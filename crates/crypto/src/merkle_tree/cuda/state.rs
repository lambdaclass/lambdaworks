// CUDA state management for Merkle tree operations
//
// Manages GPU device, memory, and kernel execution for Merkle tree building.

use cudarc::{
    driver::{safe::CudaSlice, CudaDevice, LaunchAsync, LaunchConfig},
    nvrtc::safe::Ptx,
};
use lambdaworks_gpu::cuda::abstractions::errors::CudaError;
use lambdaworks_math::field::{
    element::FieldElement, fields::fft_friendly::stark_252_prime_field::Stark252PrimeField,
};
use std::sync::Arc;

use crate::hash::poseidon::parameters::PermutationParameters;
use crate::hash::poseidon::starknet::PoseidonCairoStark252;

// Include the compiled PTX (generated by build.rs via nvcc)
const MERKLE_TREE_PTX: &str = include_str!(concat!(env!("OUT_DIR"), "/merkle_tree.ptx"));

const WARP_SIZE: usize = 32;

type Fp = FieldElement<Stark252PrimeField>;

/// CUDA field element representation (matches the u256 layout in CUDA).
///
/// The limb order must match both the Rust `UnsignedInteger<4>` layout (LSB-first)
/// and the CUDA `Fp256` / `u256` layout. Both use 4 x u64 in little-endian order,
/// so a direct copy of limbs is correct.
#[repr(C)]
#[derive(Clone, Copy, Default)]
pub struct CudaFieldElement {
    limbs: [u64; 4],
}

// SAFETY: All-zero bytes are a valid `CudaFieldElement` (represents the zero field element).
// The type contains only a `[u64; 4]` array where all zeros is a valid field element (zero).
// No pointers, references, or other types that require non-zero bit patterns.
unsafe impl cudarc::driver::safe::ValidAsZeroBits for CudaFieldElement {}

impl From<&Fp> for CudaFieldElement {
    fn from(fe: &Fp) -> Self {
        let value = fe.value();
        Self { limbs: value.limbs }
    }
}

impl From<CudaFieldElement> for Fp {
    fn from(cuda_fe: CudaFieldElement) -> Self {
        use lambdaworks_math::unsigned_integer::element::UnsignedInteger;
        Fp::from_raw(UnsignedInteger::from_limbs(cuda_fe.limbs))
    }
}

// SAFETY: `CudaFieldElement` is `#[repr(C)]` with a single `[u64; 4]` field,
// which matches the GPU-side `u256` struct layout (4 x u64, no padding).
// The pointer cast is valid because:
// 1. Type has stable, well-defined layout with #[repr(C)]
// 2. No interior mutability or aliasing concerns (simple data structure)
// 3. Pointer remains valid for the duration of the kernel launch
// 4. GPU-side struct has identical memory layout (verified by PTX compilation)
unsafe impl cudarc::driver::safe::DeviceRepr for CudaFieldElement {
    fn as_kernel_param(&self) -> *mut core::ffi::c_void {
        self as *const _ as *mut _
    }
}

/// CUDA state for Merkle tree operations
pub struct CudaMerkleState {
    device: Arc<CudaDevice>,
    round_constants: CudaSlice<CudaFieldElement>,
}

impl CudaMerkleState {
    /// Creates a new CUDA state for Merkle tree operations
    pub fn new() -> Result<Self, CudaError> {
        // CudaDevice::new returns Arc<CudaDevice>
        let device =
            CudaDevice::new(0).map_err(|err| CudaError::DeviceNotFound(err.to_string()))?;

        let functions = ["hash_leaves", "compute_parents"];
        device
            .load_ptx(Ptx::from_src(MERKLE_TREE_PTX), "merkle_tree", &functions)
            .map_err(|err| CudaError::PtxError(err.to_string()))?;

        let round_constants = Self::upload_round_constants(&device)?;

        Ok(Self {
            device,
            round_constants,
        })
    }

    fn upload_round_constants(
        device: &Arc<CudaDevice>,
    ) -> Result<CudaSlice<CudaFieldElement>, CudaError> {
        let constants: &[Fp] = PoseidonCairoStark252::ROUND_CONSTANTS;
        let cuda_constants: Vec<CudaFieldElement> =
            constants.iter().map(CudaFieldElement::from).collect();

        device
            .htod_sync_copy(&cuda_constants)
            .map_err(|err| CudaError::AllocateMemory(err.to_string()))
    }

    /// Hash leaves in parallel on the GPU
    pub fn hash_leaves(&self, leaves: &[Fp]) -> Result<Vec<Fp>, CudaError> {
        let num_leaves = leaves.len();
        if num_leaves == 0 {
            return Ok(Vec::new());
        }

        let cuda_leaves: Vec<CudaFieldElement> =
            leaves.iter().map(CudaFieldElement::from).collect();

        let input_buffer = self
            .device
            .htod_sync_copy(&cuda_leaves)
            .map_err(|err| CudaError::AllocateMemory(err.to_string()))?;

        let mut output_buffer: CudaSlice<CudaFieldElement> = self
            .device
            .alloc_zeros(num_leaves)
            .map_err(|err| CudaError::AllocateMemory(err.to_string()))?;

        let hash_leaves_fn = self
            .device
            .get_func("merkle_tree", "hash_leaves")
            .ok_or(CudaError::FunctionError("hash_leaves not found".into()))?;

        let block_size = WARP_SIZE;
        let grid_size = num_leaves.div_ceil(block_size);

        let config = LaunchConfig {
            grid_dim: (grid_size as u32, 1, 1),
            block_dim: (block_size as u32, 1, 1),
            shared_mem_bytes: 0,
        };

        // SAFETY: Kernel launch safety requirements:
        // 1. All pointers are valid CudaSlice references with correct lifetimes
        // 2. Kernel parameters match function signature (verified at PTX load time)
        // 3. Grid/block dimensions are within GPU limits (grid_size computed from input, block_size = WARP_SIZE = 32)
        // 4. No race conditions: each thread i writes to unique output[i]
        // 5. Buffer sizes are correct: input_buffer and output_buffer both have num_leaves elements
        // 6. round_constants has correct size (verified at upload time)
        unsafe {
            hash_leaves_fn.launch(
                config,
                (
                    &input_buffer,
                    &mut output_buffer,
                    &self.round_constants,
                    num_leaves as u32,
                ),
            )
        }
        .map_err(|err| CudaError::Launch(err.to_string()))?;

        let result = self
            .device
            .sync_reclaim(output_buffer)
            .map_err(|err| CudaError::RetrieveMemory(err.to_string()))?;

        Ok(result.into_iter().map(Fp::from).collect())
    }

    /// Build one layer of parents from children.
    ///
    /// `children` must have an even number of elements (each consecutive pair
    /// is hashed to produce one parent). Returns an error if length is odd.
    pub fn build_layer(&self, children: &[Fp]) -> Result<Vec<Fp>, CudaError> {
        let num_children = children.len();
        if num_children < 2 {
            return Err(CudaError::FunctionError("Need at least 2 children".into()));
        }

        // Runtime validation: ensure even number of children in all builds (not just debug)
        if num_children % 2 != 0 {
            return Err(CudaError::FunctionError(format!(
                "build_layer requires even number of children, got {}",
                num_children
            )));
        }

        let num_parents = num_children / 2;

        let cuda_children: Vec<CudaFieldElement> =
            children.iter().map(CudaFieldElement::from).collect();

        let children_buffer = self
            .device
            .htod_sync_copy(&cuda_children)
            .map_err(|err| CudaError::AllocateMemory(err.to_string()))?;

        let mut parents_buffer: CudaSlice<CudaFieldElement> = self
            .device
            .alloc_zeros(num_parents)
            .map_err(|err| CudaError::AllocateMemory(err.to_string()))?;

        let compute_parents_fn = self
            .device
            .get_func("merkle_tree", "compute_parents")
            .ok_or(CudaError::FunctionError("compute_parents not found".into()))?;

        let block_size = WARP_SIZE;
        let grid_size = num_parents.div_ceil(block_size);

        let config = LaunchConfig {
            grid_dim: (grid_size as u32, 1, 1),
            block_dim: (block_size as u32, 1, 1),
            shared_mem_bytes: 0,
        };

        // SAFETY: Kernel launch safety requirements:
        // 1. All pointers are valid CudaSlice references with correct lifetimes
        // 2. Kernel parameters match function signature (verified at PTX load time)
        // 3. Grid/block dimensions are within GPU limits (grid_size = num_parents.div_ceil(WARP_SIZE))
        // 4. No race conditions: each thread i reads children[2*i] and children[2*i+1], writes to parents[i]
        // 5. Buffer sizes are correct: children_buffer has num_children elements (even, validated),
        //    parents_buffer has num_parents = num_children/2 elements
        // 6. round_constants has correct size (verified at upload time)
        unsafe {
            compute_parents_fn.launch(
                config,
                (
                    &children_buffer,
                    &mut parents_buffer,
                    &self.round_constants,
                    num_parents as u32,
                ),
            )
        }
        .map_err(|err| CudaError::Launch(err.to_string()))?;

        let result = self
            .device
            .sync_reclaim(parents_buffer)
            .map_err(|err| CudaError::RetrieveMemory(err.to_string()))?;

        Ok(result.into_iter().map(Fp::from).collect())
    }

    /// Build entire Merkle tree from leaves.
    ///
    /// Returns `(root, all_nodes)` where `all_nodes` is in the compact format:
    /// `[root][level 1][level 2]...[leaf hashes]`, totalling `2n - 1` nodes
    /// for `n` leaves (after padding to power of 2).
    ///
    /// Leaf hashing is done via [`hash_leaves`] (which applies the GPU threshold).
    /// Tree layers are computed on the GPU without re-uploading intermediate
    /// results — only the final download of each layer is performed.
    pub fn build_tree(&self, leaves: &[Fp]) -> Result<(Fp, Vec<Fp>), CudaError> {
        if leaves.is_empty() {
            return Err(CudaError::FunctionError(
                "Cannot build tree from empty leaves".into(),
            ));
        }

        // Hash leaves (respects GPU threshold internally)
        let mut hashed = self.hash_leaves(leaves)?;

        // Pad to power of 2 by repeating the last element (matches
        // `complete_until_power_of_two` in merkle_tree::utils).
        while !hashed.len().is_power_of_two() {
            let last = hashed[hashed.len() - 1];
            hashed.push(last);
        }

        let n = hashed.len();
        if n == 1 {
            return Ok((hashed[0], hashed));
        }

        // Upload hashed leaves to GPU once
        let cuda_hashed: Vec<CudaFieldElement> =
            hashed.iter().map(CudaFieldElement::from).collect();
        let mut current_gpu = self
            .device
            .htod_sync_copy(&cuda_hashed)
            .map_err(|e| CudaError::AllocateMemory(e.to_string()))?;

        let mut all_layers: Vec<Vec<Fp>> = vec![hashed];
        let mut current_size = n;

        // Build tree bottom-up, keeping intermediate layers on GPU
        while current_size > 1 {
            let num_parents = current_size / 2;

            let mut parent_gpu: CudaSlice<CudaFieldElement> = self
                .device
                .alloc_zeros(num_parents)
                .map_err(|e| CudaError::AllocateMemory(e.to_string()))?;

            let compute_fn = self
                .device
                .get_func("merkle_tree", "compute_parents")
                .ok_or(CudaError::FunctionError("compute_parents not found".into()))?;

            let block_size = WARP_SIZE;
            let grid_size = num_parents.div_ceil(block_size);
            let config = LaunchConfig {
                grid_dim: (grid_size as u32, 1, 1),
                block_dim: (block_size as u32, 1, 1),
                shared_mem_bytes: 0,
            };

            // SAFETY: Kernel launch safety requirements (build_tree internal loop):
            // 1. All pointers are valid CudaSlice references that remain valid for kernel duration
            // 2. Kernel parameters match compute_parents signature (verified at PTX load)
            // 3. Grid/block dimensions within GPU limits (grid_size derived from num_parents)
            // 4. No race conditions: thread i computes parent[i] from current_gpu[2*i, 2*i+1]
            // 5. Buffer sizes correct: current_gpu has current_size elements (always even, power of 2),
            //    parent_gpu has num_parents = current_size/2 elements
            // 6. Loop invariant: current_size is always power of 2 (ensured by initial padding)
            unsafe {
                compute_fn.launch(
                    config,
                    (
                        &current_gpu,
                        &mut parent_gpu,
                        &self.round_constants,
                        num_parents as u32,
                    ),
                )
            }
            .map_err(|e| CudaError::Launch(e.to_string()))?;

            // Download this layer (borrow — does not consume the GPU buffer)
            let parent_host = self
                .device
                .dtoh_sync_copy(&parent_gpu)
                .map_err(|e| CudaError::RetrieveMemory(e.to_string()))?;
            let parent_fps: Vec<Fp> = parent_host.into_iter().map(Fp::from).collect();
            all_layers.push(parent_fps);

            // Next iteration reads from this GPU buffer (no re-upload)
            current_gpu = parent_gpu;
            current_size = num_parents;
        }

        let root = all_layers.last().unwrap()[0];

        // Reconstruct nodes in compact format: root-first, leaves-last
        let total = 2 * n - 1;
        let mut nodes = Vec::with_capacity(total);
        for layer in all_layers.iter().rev() {
            nodes.extend_from_slice(layer);
        }

        Ok((root, nodes))
    }
}
